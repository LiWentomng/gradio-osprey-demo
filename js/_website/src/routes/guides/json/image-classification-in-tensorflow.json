{"guide": {"name": "image-classification-in-tensorflow", "category": "integrating-other-frameworks", "pretty_category": "Integrating Other Frameworks", "guide_index": null, "absolute_index": 18, "pretty_name": "Image Classification In Tensorflow", "content": "# Image Classification in TensorFlow and Keras\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/keras-image-classifier\nTags: VISION, MOBILENET, TENSORFLOW\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from traffic control systems to satellite imaging. \n\nSuch models are perfect to use with Gradio's *image* input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n\nLet's get started!\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). We will be using a pretrained Keras image classification model, so you should also have `tensorflow` installed.\n\n## Step 1 \u2014 Setting up the Image Classification Model\n\nFirst, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from [Keras](https://keras.io/api/applications/mobilenet/). You can use a different pretrained model or train your own. \n\n```python\nimport tensorflow as tf\n\ninception_net = tf.keras.applications.MobileNetV2()\n```\n\nThis line automatically downloads the MobileNet model and weights using the Keras library.  \n\n## Step 2 \u2014 Defining a `predict` function\n\nNext, we will need to define a function that takes in the *user input*, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).\n\nIn the case of our pretrained model, it will look like this:\n\n```python\nimport requests\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n  inp = inp.reshape((-1, 224, 224, 3))\n  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n  prediction = inception_net.predict(inp).flatten()\n  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n```\n\nLet's break this down. The function takes one parameter:\n\n* `inp`: the input image as a `numpy` array\n\nThen, the function adds a batch dimension, passes it through the model, and returns:\n\n* `confidences`: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities\n\n## Step 3 \u2014 Creating a Gradio Interface\n\nNow that we have our predictive function set up, we can create a Gradio Interface around it. \n\nIn this case, the input component is a drag-and-drop image component. To create this input, we can use the `\"gradio.inputs.Image\"` class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.\n\nThe output component will be a `\"label\"`, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.\n\nFinally, we'll add one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:\n\n```python\nimport gradio as gr\n\ngr.Interface(fn=classify_image, \n             inputs=gr.Image(shape=(224, 224)),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"banana.jpg\", \"car.jpg\"]).launch()\n```\n\nThis produces the following interface, which you can try right here in your browser (try uploading your own examples!):\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n----------\n\nAnd you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting `share=True` when you `launch()` the Interface!\n\n", "html": "<h1 id=\"image-classification-in-tensorflow-and-keras\">Image Classification in TensorFlow and Keras</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>Image classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from traffic control systems to satellite imaging. </p>\n\n<p>Such models are perfect to use with Gradio's <em>image</em> input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in Python, and it will look like this (try one of the examples!):</p>\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>Let's get started!</p>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. We will be using a pretrained Keras image classification model, so you should also have <code>tensorflow</code> installed.</p>\n\n<h2 id=\"step-1-setting-up-the-image-classification-model\">Step 1 \u2014 Setting up the Image Classification Model</h2>\n\n<p>First, we will need an image classification model. For this tutorial, we will use a pretrained Mobile Net model, as it is easily downloadable from <a rel=\"noopener\" target=\"_blank\" href=\"https://keras.io/api/applications/mobilenet/\">Keras</a>. You can use a different pretrained model or train your own. </p>\n\n<div class='codeblock'><pre><code class='lang-python'>import tensorflow as tf\n\ninception_net = tf.keras.applications.MobileNetV2()\n</code></pre></div>\n\n<p>This line automatically downloads the MobileNet model and weights using the Keras library.  </p>\n\n<h2 id=\"step-2-defining-a-predict-function\">Step 2 \u2014 Defining a <code>predict</code> function</h2>\n\n<p>Next, we will need to define a function that takes in the <em>user input</em>, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this <a rel=\"noopener\" target=\"_blank\" href=\"https://git.io/JJkYN\">text file</a>.</p>\n\n<p>In the case of our pretrained model, it will look like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import requests\n\n# Download human-readable labels for ImageNet.\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\ndef classify_image(inp):\n  inp = inp.reshape((-1, 224, 224, 3))\n  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n  prediction = inception_net.predict(inp).flatten()\n  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n  return confidences\n</code></pre></div>\n\n<p>Let's break this down. The function takes one parameter:</p>\n\n<ul>\n<li><code>inp</code>: the input image as a <code>numpy</code> array</li>\n</ul>\n\n<p>Then, the function adds a batch dimension, passes it through the model, and returns:</p>\n\n<ul>\n<li><code>confidences</code>: the predictions, as a dictionary whose keys are class labels and whose values are confidence probabilities</li>\n</ul>\n\n<h2 id=\"step-3-creating-a-gradio-interface\">Step 3 \u2014 Creating a Gradio Interface</h2>\n\n<p>Now that we have our predictive function set up, we can create a Gradio Interface around it. </p>\n\n<p>In this case, the input component is a drag-and-drop image component. To create this input, we can use the <code>\"gradio.inputs.Image\"</code> class, which creates the component and handles the preprocessing to convert that to a numpy array. We will instantiate the class with a parameter that automatically preprocesses the input image to be 224 pixels by 224 pixels, which is the size that MobileNet expects.</p>\n\n<p>The output component will be a <code>\"label\"</code>, which displays the top labels in a nice form. Since we don't want to show all 1,000 class labels, we will customize it to show only the top 3 images.</p>\n\n<p>Finally, we'll add one more parameter, the <code>examples</code>, which allows us to prepopulate our interfaces with a few predefined examples. The code for Gradio looks like this:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(fn=classify_image, \n             inputs=gr.Image(shape=(224, 224)),\n             outputs=gr.Label(num_top_classes=3),\n             examples=[\"banana.jpg\", \"car.jpg\"]).launch()\n</code></pre></div>\n\n<p>This produces the following interface, which you can try right here in your browser (try uploading your own examples!):</p>\n\n<iframe src=\"https://abidlabs-keras-image-classifier.hf.space\" frameBorder=\"0\" height=\"660\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<hr />\n\n<p>And you're done! That's all the code you need to build a web demo for an image classifier. If you'd like to share with others, try setting <code>share=True</code> when you <code>launch()</code> the Interface!</p>\n", "tags": ["VISION", "MOBILENET", "TENSORFLOW"], "spaces": ["https://huggingface.co/spaces/abidlabs/keras-image-classifier"], "url": "/guides/image-classification-in-tensorflow/", "contributor": null}}