{"guide": {"name": "custom-interpretations-with-blocks", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 34, "pretty_name": "Custom Interpretations With Blocks", "content": "# Custom Machine Learning Interpretations with Blocks\nTags: INTERPRETATION, SENTIMENT ANALYSIS\n\n**Prerequisite**: This Guide requires you to know about Blocks and the interpretation feature of Interfaces.\nMake sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control) as well as the\ninterpretation section of the [Advanced Interface Features Guide](/advanced-interface-features#interpreting-your-predictions).\n\n## Introduction\n\nIf you have experience working with the Interface class, then you know that interpreting the prediction of your machine learning model\nis as easy as setting the `interpretation` parameter to either \"default\" or \"shap\".\n\nYou may be wondering if it is possible to add the same interpretation functionality to an app built with the Blocks API.\nNot only is it possible, but the flexibility of Blocks lets you display the interpretation output in ways that are\nimpossible to do with Interfaces!\n\nThis guide will show how to:\n\n1. Recreate the behavior of Interfaces's interpretation feature in a Blocks app.\n2. Customize how interpretations are displayed in a Blocks app.\n\nLet's get started!\n\n## Setting up the Blocks app\n\nLet's build a sentiment classification app with the Blocks API.\nThis app will take text as input and output the probability that this text expresses either negative or positive sentiment.\nWe'll have a single input `Textbox` and a single output `Label` component.\nBelow is the code for the app as well as the app itself.\n\n```python\nimport gradio as gr \nfrom transformers import pipeline\n\nsentiment_classifier = pipeline(\"text-classification\", return_all_scores=True)\n\ndef classifier(text):\n    pred = sentiment_classifier(text)\n    return {p[\"label\"]: p[\"score\"] for p in pred[0]}\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n\n    classify.click(classifier, input_text, label)\ndemo.launch()\n```\n\n<gradio-app space=\"freddyaboulton/sentiment-classification\"> </gradio-app>\n\n## Adding interpretations to the app\n\nOur goal is to present to our users how the words in the input contribute to the model's prediction.\nThis will help our users understand how the model works and also evaluate its effectiveness.\nFor example, we should expect our model to identify the words \"happy\" and \"love\" with positive sentiment - if not it's a sign we made a mistake in training it!\n\nFor each word in the input, we will compute a score of how much the model's prediction of positive sentiment is changed by that word.\nOnce we have those `(word, score)` pairs we can use gradio to visualize them for the user.\n\nThe [shap](https://shap.readthedocs.io/en/stable/index.html) library will help us compute the `(word, score)` pairs and\ngradio will take care of displaying the output to the user.\n\nThe following code computes the `(word, score)` pairs:\n\n```python\ndef interpretation_function(text):\n    explainer = shap.Explainer(sentiment_classifier)\n    shap_values = explainer([text])\n    \n    # Dimensions are (batch size, text size, number of classes)\n    # Since we care about positive sentiment, use index 1\n    scores = list(zip(shap_values.data[0], shap_values.values[0, :, 1]))\n    # Scores contains (word, score) pairs\n    \n    \n    # Format expected by gr.components.Interpretation\n    return {\"original\": text, \"interpretation\": scores}\n```\n\nNow, all we have to do is add a button that runs this function when clicked.\nTo display the interpretation, we will use `gr.components.Interpretation`.\nThis will color each word in the input either red or blue.\nRed if it contributes to positive sentiment and blue if it contributes to negative sentiment.\nThis is how `Interface` displays the interpretation output for text.\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n                interpret = gr.Button(\"Interpret\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n        with gr.Column():\n            interpretation = gr.components.Interpretation(input_text)\n    classify.click(classifier, input_text, label)\n    interpret.click(interpretation_function, input_text, interpretation)\n\ndemo.launch()\n```\n\n<gradio-app space=\"freddyaboulton/sentiment-classification-interpretation\"> </gradio-app>\n\n\n## Customizing how the interpretation is displayed\n\nThe `gr.components.Interpretation` component does a good job of showing how individual words contribute to the sentiment prediction,\nbut what if we also wanted to display the score themselves along with the words?\n\nOne way to do this would be to generate a bar plot where the words are on the horizontal axis and the bar height corresponds\nto the shap score.\n\nWe can do this by modifying our `interpretation_function` to additionally return a matplotlib bar plot.\nWe will display it with the `gr.Plot` component in a separate tab.\n\nThis is how the interpretation function will look:\n```python\ndef interpretation_function(text):\n    explainer = shap.Explainer(sentiment_classifier)\n    shap_values = explainer([text])\n    # Dimensions are (batch size, text size, number of classes)\n    # Since we care about positive sentiment, use index 1\n    scores = list(zip(shap_values.data[0], shap_values.values[0, :, 1]))\n\n    scores_desc = sorted(scores, key=lambda t: t[1])[::-1]\n\n    # Filter out empty string added by shap\n    scores_desc = [t for t in scores_desc if t[0] != \"\"]\n\n    fig_m = plt.figure()\n    \n    # Select top 5 words that contribute to positive sentiment\n    plt.bar(x=[s[0] for s in scores_desc[:5]],\n            height=[s[1] for s in scores_desc[:5]])\n    plt.title(\"Top words contributing to positive sentiment\")\n    plt.ylabel(\"Shap Value\")\n    plt.xlabel(\"Word\")\n    return {\"original\": text, \"interpretation\": scores}, fig_m\n```\n\nAnd this is how the app code will look:\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n                interpret = gr.Button(\"Interpret\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n        with gr.Column():\n            with gr.Tabs():\n                with gr.TabItem(\"Display interpretation with built-in component\"):\n                    interpretation = gr.components.Interpretation(input_text)\n                with gr.TabItem(\"Display interpretation with plot\"):\n                    interpretation_plot = gr.Plot()\n\n    classify.click(classifier, input_text, label)\n    interpret.click(interpretation_function, input_text, [interpretation, interpretation_plot])\n\ndemo.launch()\n```\n\nYou can see the demo below!\n\n<gradio-app space=\"freddyaboulton/sentiment-classification-interpretation-tabs\"> </gradio-app>\n\n## Beyond Sentiment Classification\nAlthough we have focused on sentiment classification so far, you can add interpretations to almost any machine learning model.\nThe output must be an `gr.Image` or `gr.Label` but the input can be almost anything (`gr.Number`, `gr.Slider`, `gr.Radio`, `gr.Image`).\n\nHere is a demo built with blocks of interpretations for an image classification model:\n\n<gradio-app space=\"freddyaboulton/image-classification-interpretation-blocks\"> </gradio-app>\n\n\n## Closing remarks\n\nWe did a deep dive \ud83e\udd3f into how interpretations work and how you can add them to your Blocks app.\n\nWe also showed how the Blocks API gives you the power to control how the interpretation is visualized in your app.\n\nAdding interpretations is a helpful way to make your users understand and gain trust in your model.\nNow you have all the tools you need to add them to all of your apps!\n", "html": "<h1 id=\"custom-machine-learning-interpretations-with-blocks\">Custom Machine Learning Interpretations with Blocks</h1>\n\n<p><strong>Prerequisite</strong>: This Guide requires you to know about Blocks and the interpretation feature of Interfaces.\nMake sure to <a rel=\"noopener\" target=\"_blank\" href=\"https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control\">read the Guide to Blocks first</a> as well as the\ninterpretation section of the <a rel=\"noopener\" target=\"_blank\" href=\"/advanced-interface-features#interpreting-your-predictions\">Advanced Interface Features Guide</a>.</p>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>If you have experience working with the Interface class, then you know that interpreting the prediction of your machine learning model\nis as easy as setting the <code>interpretation</code> parameter to either \"default\" or \"shap\".</p>\n\n<p>You may be wondering if it is possible to add the same interpretation functionality to an app built with the Blocks API.\nNot only is it possible, but the flexibility of Blocks lets you display the interpretation output in ways that are\nimpossible to do with Interfaces!</p>\n\n<p>This guide will show how to:</p>\n\n<ol>\n<li>Recreate the behavior of Interfaces's interpretation feature in a Blocks app.</li>\n<li>Customize how interpretations are displayed in a Blocks app.</li>\n</ol>\n\n<p>Let's get started!</p>\n\n<h2 id=\"setting-up-the-blocks-app\">Setting up the Blocks app</h2>\n\n<p>Let's build a sentiment classification app with the Blocks API.\nThis app will take text as input and output the probability that this text expresses either negative or positive sentiment.\nWe'll have a single input <code>Textbox</code> and a single output <code>Label</code> component.\nBelow is the code for the app as well as the app itself.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr \nfrom transformers import pipeline\n\nsentiment_classifier = pipeline(\"text-classification\", return_all_scores=True)\n\ndef classifier(text):\n    pred = sentiment_classifier(text)\n    return {p[\"label\"]: p[\"score\"] for p in pred[0]}\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n\n    classify.click(classifier, input_text, label)\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space=\"freddyaboulton/sentiment-classification\"> </gradio-app></p>\n\n<h2 id=\"adding-interpretations-to-the-app\">Adding interpretations to the app</h2>\n\n<p>Our goal is to present to our users how the words in the input contribute to the model's prediction.\nThis will help our users understand how the model works and also evaluate its effectiveness.\nFor example, we should expect our model to identify the words \"happy\" and \"love\" with positive sentiment - if not it's a sign we made a mistake in training it!</p>\n\n<p>For each word in the input, we will compute a score of how much the model's prediction of positive sentiment is changed by that word.\nOnce we have those <code>(word, score)</code> pairs we can use gradio to visualize them for the user.</p>\n\n<p>The <a rel=\"noopener\" target=\"_blank\" href=\"https://shap.readthedocs.io/en/stable/index.html\">shap</a> library will help us compute the <code>(word, score)</code> pairs and\ngradio will take care of displaying the output to the user.</p>\n\n<p>The following code computes the <code>(word, score)</code> pairs:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def interpretation_function(text):\n    explainer = shap.Explainer(sentiment_classifier)\n    shap_values = explainer([text])\n\n    # Dimensions are (batch size, text size, number of classes)\n    # Since we care about positive sentiment, use index 1\n    scores = list(zip(shap_values.data[0], shap_values.values[0, :, 1]))\n    # Scores contains (word, score) pairs\n\n\n    # Format expected by gr.components.Interpretation\n    return {\"original\": text, \"interpretation\": scores}\n</code></pre></div>\n\n<p>Now, all we have to do is add a button that runs this function when clicked.\nTo display the interpretation, we will use <code>gr.components.Interpretation</code>.\nThis will color each word in the input either red or blue.\nRed if it contributes to positive sentiment and blue if it contributes to negative sentiment.\nThis is how <code>Interface</code> displays the interpretation output for text.</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n                interpret = gr.Button(\"Interpret\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n        with gr.Column():\n            interpretation = gr.components.Interpretation(input_text)\n    classify.click(classifier, input_text, label)\n    interpret.click(interpretation_function, input_text, interpretation)\n\ndemo.launch()\n</code></pre></div>\n\n<p><gradio-app space=\"freddyaboulton/sentiment-classification-interpretation\"> </gradio-app></p>\n\n<h2 id=\"customizing-how-the-interpretation-is-displayed\">Customizing how the interpretation is displayed</h2>\n\n<p>The <code>gr.components.Interpretation</code> component does a good job of showing how individual words contribute to the sentiment prediction,\nbut what if we also wanted to display the score themselves along with the words?</p>\n\n<p>One way to do this would be to generate a bar plot where the words are on the horizontal axis and the bar height corresponds\nto the shap score.</p>\n\n<p>We can do this by modifying our <code>interpretation_function</code> to additionally return a matplotlib bar plot.\nWe will display it with the <code>gr.Plot</code> component in a separate tab.</p>\n\n<p>This is how the interpretation function will look:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def interpretation_function(text):\n    explainer = shap.Explainer(sentiment_classifier)\n    shap_values = explainer([text])\n    # Dimensions are (batch size, text size, number of classes)\n    # Since we care about positive sentiment, use index 1\n    scores = list(zip(shap_values.data[0], shap_values.values[0, :, 1]))\n\n    scores_desc = sorted(scores, key=lambda t: t[1])[::-1]\n\n    # Filter out empty string added by shap\n    scores_desc = [t for t in scores_desc if t[0] != \"\"]\n\n    fig_m = plt.figure()\n\n    # Select top 5 words that contribute to positive sentiment\n    plt.bar(x=[s[0] for s in scores_desc[:5]],\n            height=[s[1] for s in scores_desc[:5]])\n    plt.title(\"Top words contributing to positive sentiment\")\n    plt.ylabel(\"Shap Value\")\n    plt.xlabel(\"Word\")\n    return {\"original\": text, \"interpretation\": scores}, fig_m\n</code></pre></div>\n\n<p>And this is how the app code will look:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>with gr.Blocks() as demo:\n    with gr.Row():\n        with gr.Column():\n            input_text = gr.Textbox(label=\"Input Text\")\n            with gr.Row():\n                classify = gr.Button(\"Classify Sentiment\")\n                interpret = gr.Button(\"Interpret\")\n        with gr.Column():\n            label = gr.Label(label=\"Predicted Sentiment\")\n        with gr.Column():\n            with gr.Tabs():\n                with gr.TabItem(\"Display interpretation with built-in component\"):\n                    interpretation = gr.components.Interpretation(input_text)\n                with gr.TabItem(\"Display interpretation with plot\"):\n                    interpretation_plot = gr.Plot()\n\n    classify.click(classifier, input_text, label)\n    interpret.click(interpretation_function, input_text, [interpretation, interpretation_plot])\n\ndemo.launch()\n</code></pre></div>\n\n<p>You can see the demo below!</p>\n\n<p><gradio-app space=\"freddyaboulton/sentiment-classification-interpretation-tabs\"> </gradio-app></p>\n\n<h2 id=\"beyond-sentiment-classification\">Beyond Sentiment Classification</h2>\n\n<p>Although we have focused on sentiment classification so far, you can add interpretations to almost any machine learning model.\nThe output must be an <code>gr.Image</code> or <code>gr.Label</code> but the input can be almost anything (<code>gr.Number</code>, <code>gr.Slider</code>, <code>gr.Radio</code>, <code>gr.Image</code>).</p>\n\n<p>Here is a demo built with blocks of interpretations for an image classification model:</p>\n\n<p><gradio-app space=\"freddyaboulton/image-classification-interpretation-blocks\"> </gradio-app></p>\n\n<h2 id=\"closing-remarks\">Closing remarks</h2>\n\n<p>We did a deep dive \ud83e\udd3f into how interpretations work and how you can add them to your Blocks app.</p>\n\n<p>We also showed how the Blocks API gives you the power to control how the interpretation is visualized in your app.</p>\n\n<p>Adding interpretations is a helpful way to make your users understand and gain trust in your model.\nNow you have all the tools you need to add them to all of your apps!</p>\n", "tags": ["INTERPRETATION", "SENTIMENT ANALYSIS"], "spaces": [], "url": "/guides/custom-interpretations-with-blocks/", "contributor": null}}