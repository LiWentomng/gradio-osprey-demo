{"guide": {"name": "create-your-own-friends-with-a-gan", "category": "other-tutorials", "pretty_category": "Other Tutorials", "guide_index": null, "absolute_index": 31, "pretty_name": "Create Your Own Friends With A Gan", "content": "# Create Your Own Friends with a GAN\n\nRelated spaces: https://huggingface.co/spaces/NimaBoscarino/cryptopunks, https://huggingface.co/spaces/nateraw/cryptopunks-generator\nTags: GAN, IMAGE, HUB\n\nContributed by <a href=\"https://huggingface.co/NimaBoscarino\">Nima Boscarino</a> and <a href=\"https://huggingface.co/nateraw\">Nate Raw</a>\n\n\n## Introduction\n\nIt seems that cryptocurrencies, [NFTs](https://www.nytimes.com/interactive/2022/03/18/technology/nft-guide.html), and the web3 movement are all the rage these days! Digital assets are being listed on marketplaces for astounding amounts of money, and just about every celebrity is debuting their own NFT collection. While your crypto assets [may be taxable, such as in Canada](https://www.canada.ca/en/revenue-agency/programs/about-canada-revenue-agency-cra/compliance/digital-currency/cryptocurrency-guide.html), today we'll explore some fun and tax-free ways to generate your own assortment of procedurally generated [CryptoPunks](https://www.larvalabs.com/cryptopunks).\n\nGenerative Adversarial Networks, often known just as *GANs*, are a specific class of deep-learning models that are designed to learn from an input dataset to create (*generate!*) new material that is convincingly similar to elements of the original training set. Famously, the website [thispersondoesnotexist.com](https://thispersondoesnotexist.com/) went viral with lifelike, yet synthetic, images of people generated with a model called StyleGAN2. GANs have gained traction in the machine learning world, and are now being used to generate all sorts of images, text, and even [music](https://salu133445.github.io/musegan/)!\n\nToday we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo around a pre-trained GAN to see what all the fuss is about. Here's a peek at what we're going to be putting together:\n\n<iframe src=\"https://nimaboscarino-cryptopunks.hf.space\" frameBorder=\"0\" height=\"855\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n### Prerequisites\n\nMake sure you have the `gradio` Python package already [installed](/getting_started). To use the pretrained model, also install `torch` and `torchvision`.\n\n## GANs: a very brief introduction\n\nOriginally proposed in [Goodfellow et al. 2014](https://arxiv.org/abs/1406.2661), GANs are made up of neural networks which compete with the intention of outsmarting each other. One network, known as the *generator*, is responsible for generating images. The other network, the *discriminator*, receives an image at a time from the generator along with a **real** image from the training data set. The discriminator then has to guess: which image is the fake?\n\nThe generator is constantly training to create images which are trickier for the discriminator to identify, while the discriminator raises the bar for the generator every time it correctly detects a fake. As the networks engage in this competitive (*adversarial!*) relationship, the images that get generated improve to the point where they become indistinguishable to human eyes!\n\nFor a more in-depth look at GANs, you can take a look at [this excellent post on Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/06/a-detailed-explanation-of-gan-with-implementation-using-tensorflow-and-keras/) or this [PyTorch tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html). For now, though, we'll dive into a demo!\n\n## Step 1 \u2014 Create the Generator model\n\nTo generate new images with a GAN, you only need the generator model. There are many different architectures that the generator could use, but for this demo we'll use a pretrained GAN generator model with the following architecture:\n\n```python\nfrom torch import nn\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n```\n\nWe're taking the generator from [this repo by @teddykoker](https://github.com/teddykoker/cryptopunks-gan/blob/main/train.py#L90), where you can also see the original discriminator model structure.\n\nAfter instantiating the model, we'll load in the weights from the Hugging Face Hub, stored at [nateraw/cryptopunks-gan](https://huggingface.co/nateraw/cryptopunks-gan):\n\n```python\nfrom huggingface_hub import hf_hub_download\nimport torch\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n```\n\n## Step 2 \u2014 Defining a `predict` function\n\nThe `predict` function is the key to making Gradio work! Whatever inputs we choose through the Gradio interface will get passed through our `predict` function, which should operate on the inputs and generate outputs that we can display with Gradio output components. For GANs it's common to pass random noise into our model as the input, so we'll generate a tensor of random numbers and pass that through the model. We can then use `torchvision`'s `save_image` function to save the output of the model as a `png` file, and return the file name:\n\n```python\nfrom torchvision.utils import save_image\n\ndef predict(seed):\n    num_punks = 4\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWe're giving our `predict` function a `seed` parameter, so that we can fix the random tensor generation with a seed. We'll then be able to reproduce punks if we want to see them again by passing in the same seed.\n\n*Note!* Our model needs an input tensor of dimensions 100x1x1 to do a single inference, or (BatchSize)x100x1x1 for generating a batch of images. In this demo we'll start by generating 4 punks at a time.\n\n## Step 3 \u2014 Creating a Gradio interface\n\nAt this point you can even run the code you have with `predict(<SOME_NUMBER>)`, and you'll find your freshly generated punks in your file system at `./punks.png`. To make a truly interactive demo, though, we'll build out a simple interface with Gradio. Our goals here are to:\n\n* Set a slider input so users can choose the \"seed\" value\n* Use an image component for our output to showcase the generated punks\n* Use our `predict()` to take the seed and generate the images\n\nWith `gr.Interface()`, we can define all of that with a single function call:\n\n```python\nimport gradio as gr\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n    ],\n    outputs=\"image\",\n).launch()\n```\n\nLaunching the interface should present you with something like this:\n\n<iframe src=\"https://nimaboscarino-cryptopunks-1.hf.space\" frameBorder=\"0\" height=\"365\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n## Step 4 \u2014 Even more punks!\n\nGenerating 4 punks at a time is a good start, but maybe we'd like to control how many we want to make each time. Adding more inputs to our Gradio interface is as simple as adding another item to the `inputs` list that we pass to `gr.Interface`:\n\n```python\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10), # Adding another slider!\n    ],\n    outputs=\"image\",\n).launch()\n```\n\nThe new input will be passed to our `predict()` function, so we have to make some changes to that function to accept a new parameter:\n\n```python\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n```\n\nWhen you relaunch your interface, you should see a second slider that'll let you control the number of punks!\n\n## Step 5 - Polishing it up\n\nYour Gradio app is pretty much good to go, but you can add a few extra things to really make it ready for the spotlight \u2728\n\nWe can add some examples that users can easily try out by adding this to the `gr.Interface`:\n\n```python\ngr.Interface(\n    # ...\n    # keep everything as it is, and then add\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True) # cache_examples is optional\n```\n\nThe `examples` parameter takes a list of lists, where each item in the sublists is ordered in the same order that we've listed the `inputs`. So in our case, `[seed, num_punks]`. Give it a try!\n\nYou can also try adding a `title`, `description`, and `article` to the `gr.Interface`. Each of those parameters accepts a string, so try it out and see what happens \ud83d\udc40 `article` will also accept HTML, as [explored in a previous guide](/guides/key-features/#descriptive-content)!\n\nWhen you're all done, you may end up with something like this:\n\n<iframe src=\"https://nimaboscarino-cryptopunks.hf.space\" frameBorder=\"0\" height=\"855\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\nFor reference, here is our full code:\n\n```python\nimport torch\nfrom torch import nn\nfrom huggingface_hub import hf_hub_download\nfrom torchvision.utils import save_image\nimport gradio as gr\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10),\n    ],\n    outputs=\"image\",\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True)\n```\n----------\n\nCongratulations! You've built out your very own GAN-powered CryptoPunks generator, with a fancy Gradio interface that makes it easy for anyone to use. Now you can [scour the Hub for more GANs](https://huggingface.co/models?other=gan) (or train your own) and continue making even more awesome demos \ud83e\udd17", "html": "<h1 id=\"create-your-own-friends-with-a-gan\">Create Your Own Friends with a GAN</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>It seems that cryptocurrencies, <a rel=\"noopener\" target=\"_blank\" href=\"https://www.nytimes.com/interactive/2022/03/18/technology/nft-guide.html\">NFTs</a>, and the web3 movement are all the rage these days! Digital assets are being listed on marketplaces for astounding amounts of money, and just about every celebrity is debuting their own NFT collection. While your crypto assets <a rel=\"noopener\" target=\"_blank\" href=\"https://www.canada.ca/en/revenue-agency/programs/about-canada-revenue-agency-cra/compliance/digital-currency/cryptocurrency-guide.html\">may be taxable, such as in Canada</a>, today we'll explore some fun and tax-free ways to generate your own assortment of procedurally generated <a rel=\"noopener\" target=\"_blank\" href=\"https://www.larvalabs.com/cryptopunks\">CryptoPunks</a>.</p>\n\n<p>Generative Adversarial Networks, often known just as <em>GANs</em>, are a specific class of deep-learning models that are designed to learn from an input dataset to create (<em>generate!</em>) new material that is convincingly similar to elements of the original training set. Famously, the website <a rel=\"noopener\" target=\"_blank\" href=\"https://thispersondoesnotexist.com/\">thispersondoesnotexist.com</a> went viral with lifelike, yet synthetic, images of people generated with a model called StyleGAN2. GANs have gained traction in the machine learning world, and are now being used to generate all sorts of images, text, and even <a rel=\"noopener\" target=\"_blank\" href=\"https://salu133445.github.io/musegan/\">music</a>!</p>\n\n<p>Today we'll briefly look at the high-level intuition behind GANs, and then we'll build a small demo around a pre-trained GAN to see what all the fuss is about. Here's a peek at what we're going to be putting together:</p>\n\n<iframe src=\"https://nimaboscarino-cryptopunks.hf.space\" frameBorder=\"0\" height=\"855\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h3 id=\"prerequisites\">Prerequisites</h3>\n\n<p>Make sure you have the <code>gradio</code> Python package already <a rel=\"noopener\" target=\"_blank\" href=\"/getting_started\">installed</a>. To use the pretrained model, also install <code>torch</code> and <code>torchvision</code>.</p>\n\n<h2 id=\"gans-a-very-brief-introduction\">GANs: a very brief introduction</h2>\n\n<p>Originally proposed in <a rel=\"noopener\" target=\"_blank\" href=\"https://arxiv.org/abs/1406.2661\">Goodfellow et al. 2014</a>, GANs are made up of neural networks which compete with the intention of outsmarting each other. One network, known as the <em>generator</em>, is responsible for generating images. The other network, the <em>discriminator</em>, receives an image at a time from the generator along with a <strong>real</strong> image from the training data set. The discriminator then has to guess: which image is the fake?</p>\n\n<p>The generator is constantly training to create images which are trickier for the discriminator to identify, while the discriminator raises the bar for the generator every time it correctly detects a fake. As the networks engage in this competitive (<em>adversarial!</em>) relationship, the images that get generated improve to the point where they become indistinguishable to human eyes!</p>\n\n<p>For a more in-depth look at GANs, you can take a look at <a rel=\"noopener\" target=\"_blank\" href=\"https://www.analyticsvidhya.com/blog/2021/06/a-detailed-explanation-of-gan-with-implementation-using-tensorflow-and-keras/\">this excellent post on Analytics Vidhya</a> or this <a rel=\"noopener\" target=\"_blank\" href=\"https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\">PyTorch tutorial</a>. For now, though, we'll dive into a demo!</p>\n\n<h2 id=\"step-1-create-the-generator-model\">Step 1 \u2014 Create the Generator model</h2>\n\n<p>To generate new images with a GAN, you only need the generator model. There are many different architectures that the generator could use, but for this demo we'll use a pretrained GAN generator model with the following architecture:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from torch import nn\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n</code></pre></div>\n\n<p>We're taking the generator from <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/teddykoker/cryptopunks-gan/blob/main/train.py#L90\">this repo by @teddykoker</a>, where you can also see the original discriminator model structure.</p>\n\n<p>After instantiating the model, we'll load in the weights from the Hugging Face Hub, stored at <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/nateraw/cryptopunks-gan\">nateraw/cryptopunks-gan</a>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from huggingface_hub import hf_hub_download\nimport torch\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n</code></pre></div>\n\n<h2 id=\"step-2-defining-a-predict-function\">Step 2 \u2014 Defining a <code>predict</code> function</h2>\n\n<p>The <code>predict</code> function is the key to making Gradio work! Whatever inputs we choose through the Gradio interface will get passed through our <code>predict</code> function, which should operate on the inputs and generate outputs that we can display with Gradio output components. For GANs it's common to pass random noise into our model as the input, so we'll generate a tensor of random numbers and pass that through the model. We can then use <code>torchvision</code>'s <code>save_image</code> function to save the output of the model as a <code>png</code> file, and return the file name:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>from torchvision.utils import save_image\n\ndef predict(seed):\n    num_punks = 4\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n</code></pre></div>\n\n<p>We're giving our <code>predict</code> function a <code>seed</code> parameter, so that we can fix the random tensor generation with a seed. We'll then be able to reproduce punks if we want to see them again by passing in the same seed.</p>\n\n<p><em>Note!</em> Our model needs an input tensor of dimensions 100x1x1 to do a single inference, or (BatchSize)x100x1x1 for generating a batch of images. In this demo we'll start by generating 4 punks at a time.</p>\n\n<h2 id=\"step-3-creating-a-gradio-interface\">Step 3 \u2014 Creating a Gradio interface</h2>\n\n<p>At this point you can even run the code you have with <code>predict(&lt;SOME_NUMBER&gt;)</code>, and you'll find your freshly generated punks in your file system at <code>./punks.png</code>. To make a truly interactive demo, though, we'll build out a simple interface with Gradio. Our goals here are to:</p>\n\n<ul>\n<li>Set a slider input so users can choose the \"seed\" value</li>\n<li>Use an image component for our output to showcase the generated punks</li>\n<li>Use our <code>predict()</code> to take the seed and generate the images</li>\n</ul>\n\n<p>With <code>gr.Interface()</code>, we can define all of that with a single function call:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import gradio as gr\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n    ],\n    outputs=\"image\",\n).launch()\n</code></pre></div>\n\n<p>Launching the interface should present you with something like this:</p>\n\n<iframe src=\"https://nimaboscarino-cryptopunks-1.hf.space\" frameBorder=\"0\" height=\"365\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<h2 id=\"step-4-even-more-punks\">Step 4 \u2014 Even more punks!</h2>\n\n<p>Generating 4 punks at a time is a good start, but maybe we'd like to control how many we want to make each time. Adding more inputs to our Gradio interface is as simple as adding another item to the <code>inputs</code> list that we pass to <code>gr.Interface</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10), # Adding another slider!\n    ],\n    outputs=\"image\",\n).launch()\n</code></pre></div>\n\n<p>The new input will be passed to our <code>predict()</code> function, so we have to make some changes to that function to accept a new parameter:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>def predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n</code></pre></div>\n\n<p>When you relaunch your interface, you should see a second slider that'll let you control the number of punks!</p>\n\n<h2 id=\"step-5-polishing-it-up\">Step 5 - Polishing it up</h2>\n\n<p>Your Gradio app is pretty much good to go, but you can add a few extra things to really make it ready for the spotlight \u2728</p>\n\n<p>We can add some examples that users can easily try out by adding this to the <code>gr.Interface</code>:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>gr.Interface(\n    # ...\n    # keep everything as it is, and then add\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True) # cache_examples is optional\n</code></pre></div>\n\n<p>The <code>examples</code> parameter takes a list of lists, where each item in the sublists is ordered in the same order that we've listed the <code>inputs</code>. So in our case, <code>[seed, num_punks]</code>. Give it a try!</p>\n\n<p>You can also try adding a <code>title</code>, <code>description</code>, and <code>article</code> to the <code>gr.Interface</code>. Each of those parameters accepts a string, so try it out and see what happens \ud83d\udc40 <code>article</code> will also accept HTML, as <a rel=\"noopener\" target=\"_blank\" href=\"/guides/key-features/#descriptive-content\">explored in a previous guide</a>!</p>\n\n<p>When you're all done, you may end up with something like this:</p>\n\n<iframe src=\"https://nimaboscarino-cryptopunks.hf.space\" frameBorder=\"0\" height=\"855\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\n\n<p>For reference, here is our full code:</p>\n\n<div class='codeblock'><pre><code class='lang-python'>import torch\nfrom torch import nn\nfrom huggingface_hub import hf_hub_download\nfrom torchvision.utils import save_image\nimport gradio as gr\n\nclass Generator(nn.Module):\n    # Refer to the link below for explanations about nc, nz, and ngf\n    # https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#inputs\n    def __init__(self, nc=4, nz=100, ngf=64):\n        super(Generator, self).__init__()\n        self.network = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 0, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh(),\n        )\n\n    def forward(self, input):\n        output = self.network(input)\n        return output\n\nmodel = Generator()\nweights_path = hf_hub_download('nateraw/cryptopunks-gan', 'generator.pth')\nmodel.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))) # Use 'cuda' if you have a GPU available\n\ndef predict(seed, num_punks):\n    torch.manual_seed(seed)\n    z = torch.randn(num_punks, 100, 1, 1)\n    punks = model(z)\n    save_image(punks, \"punks.png\", normalize=True)\n    return 'punks.png'\n\ngr.Interface(\n    predict,\n    inputs=[\n        gr.Slider(0, 1000, label='Seed', default=42),\n        gr.Slider(4, 64, label='Number of Punks', step=1, default=10),\n    ],\n    outputs=\"image\",\n    examples=[[123, 15], [42, 29], [456, 8], [1337, 35]],\n).launch(cache_examples=True)\n</code></pre></div>\n\n<hr />\n\n<p>Congratulations! You've built out your very own GAN-powered CryptoPunks generator, with a fancy Gradio interface that makes it easy for anyone to use. Now you can <a rel=\"noopener\" target=\"_blank\" href=\"https://huggingface.co/models?other=gan\">scour the Hub for more GANs</a> (or train your own) and continue making even more awesome demos \ud83e\udd17</p>\n", "tags": ["GAN", "IMAGE", "HUB"], "spaces": ["https://huggingface.co/spaces/NimaBoscarino/cryptopunks", "https://huggingface.co/spaces/nateraw/cryptopunks-generator"], "url": "/guides/create-your-own-friends-with-a-gan/", "contributor": "<a href=\"https://huggingface.co/NimaBoscarino\">Nima Boscarino</a> and <a href=\"https://huggingface.co/nateraw\">Nate Raw</a>"}}